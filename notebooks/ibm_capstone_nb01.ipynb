{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving public policy by predicting car accident severity using machine learning\n",
    "---\n",
    "\n",
    "**Disclaimer:** _This notebook is part of the Coursera Capstone Project to complete the [IBM Certification in Data Science](https://www.coursera.org/professional-certificates/ibm-data-science). All project ideas are fictitious and serve only the purpose of developing a data science project._\n",
    "\n",
    "\n",
    "Original dataset: [link_dataset](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv)  \n",
    "Metadata: [link_metadata](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Metadata.pdf)\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "---\n",
    "### 1 - [Introduction/Business Problem](#business-und)\n",
    "\n",
    "### 2 - [Data understanding and Data Preparation](#data-und-data-prep)\n",
    "\n",
    "### 3 - [Modeling](#modeling)\n",
    "\n",
    "### 4 - [Evaluation](#evaluation)\n",
    "\n",
    "### 5 - [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"business-und\"><a/>\n",
    "\n",
    "# 1 - Introduction/Business Problem\n",
    "---- \n",
    "\n",
    "\n",
    "Predicting accident severity in US cities based on specific metrics can be a powerful tool to drive public policy and reduce overall accident rate. In Seattle – Washington, the Department of Transport/Traffic Management Division,  has been collecting data since 2004 about collisions in the metropolitan area with the objective of creating a complete database that represents the overall road accidents involving collisions in this city. The City Council of Seattle has the responsibility of approving the city's budget, and develops laws and policies intended to promote safety of Seattle's residents  . During every fiscal year, the city council discusses policies to improve road safety in Seattle by reducing the number of human injuries involved in those accidents. \n",
    "    \n",
    "In this data science project, I proposed to develop a model that can distinguish accidents resulting in human injuries from accidents resulting in property damage-only. This prediction will be based on widely available metrics provided by the Department of transport and it can help to identify which factors may increase the risk for injury-related accidents and help develop actions to reduce those. The successful outcome of this project would be a model that can predict with accuracy accidents associated with human costs (i.e. high true positive rate), so that actions can be developed to minimize those costs. Examples of actions could include target advertising for road safety, improve road design, increase police deployment to secure roads, increase fines for reckless driving, among others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import necessary packages\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-dark')\n",
    "plt.rcParams['axes.titlesize'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the location of the data\n",
    "data_url = 'https://github.com/joseferncruz/coursera-capstone/raw/master/datasets/collisions_data.csv'\n",
    "\n",
    "load the data into a dataframe\n",
    "df_raw = pd.read_csv(\n",
    "    data_url,\n",
    "    parse_dates=['INCDTTM', 'INCDATE'], # Parse dates to datetime objects \n",
    "    usecols=[\n",
    "        'SEVERITYCODE', 'X', 'Y', 'ADDRTYPE', 'PERSONCOUNT', 'PEDCOUNT',\n",
    "        'PEDCYLCOUNT', 'VEHCOUNT', 'INCDTTM', 'JUNCTIONTYPE', 'WEATHER',\n",
    "        'ROADCOND', 'LIGHTCOND', 'CROSSWALKKEY', 'INCDATE',\n",
    "    ],\n",
    "    low_memory=False,\n",
    ");\n",
    "\n",
    "df_raw.drop(columns='INCDATE', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-und-data-prep\"><a/>\n",
    "\n",
    "# 2 - Data understanding and Data Preparation\n",
    "---\n",
    "    \n",
    "To develop my data project, I will use data about collisions provided by Seattle Police Department and recorded by Traffic Records . This dataset includes all types of collisions happening at intersection or mid-block of a road segment since 2004 and contains information about many important factors such as road condition and lightning conditions, weather, segment of the road involved (among other) and associated with each accident there is a variable that represents the outcome severity with 2 values: type 1 – property damage-only and type 2 – Injury-related. By using this information, I will develop a classification model aiming at predicting the severity outcome of the accident, with particular emphasis at predicting type 2 accidents.  I will focus on optimizing the model to get a high true positive rate of detection of type 2 accidents and the results obtained from this model could guide actions to decrease the occurrence of these accidents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this classification problem, I will use the following features from the original dataset:\n",
    "\n",
    "\n",
    "|Attributes| Description|  \n",
    "|---:|:----|  \n",
    "|SEVERITYCODE|A code that corresponds to the severity of the collision|  \n",
    "| X | GPS Longitude coordinate |\n",
    "| Y | GPS Latitude coordinate | \n",
    "|ADDRTYPE|Collision address type|\n",
    "|PERSONCOUNT|The total number of people involved in the collision|\n",
    "|PEDCOUNT|The number of pedestrians involved in the collision |\n",
    "|PEDCYLCOUNT| The number of bicycles involved in the collision. |\n",
    "|VEHCOUNT|The number of vehicles involved in the collision|\n",
    "|INCDTTM|The date and time of the incident.|\n",
    "| JUNCTIONTYPE| Category of junction at which collision took place |\n",
    "|WEATHER|A description of the weather conditions during the time of the collision|\n",
    "|ROADCOND|The condition of the road during the collision|\n",
    "|LIGHTCOND|The light conditions during the collision|\n",
    "|CROSSWALKKEY| A key for the crosswalk at which the collision occurred |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by exploring the features and characteristics of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This dataset has {df_raw.shape[0]} rows and {df_raw.shape[1]} columns, including one target column.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not used in this case study\n",
    "df = df_raw.copy()\n",
    "\n",
    "# display some information about the datatypes and number of entries associated with the full raw dataset\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By exploring the metadata, it is possible to notice that some columns have large quantities of missing data or lack information (Not Enough Information or NEI). Also our model should be able to predict accident severity based on features (ie information) that can be measured in real time or within an hour range (eg weather, road condition, etc).\n",
    "\n",
    "The first step is to deal with the missing/unknown information from columns in order to reduce the uncertainty around certain features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries with missing information\n",
    "\n",
    "# Remove missing/uncertain values from WEATHER\n",
    "df['WEATHER'].replace(['Unknown', 'Other'], 'uncertain', inplace=True)\n",
    "df['WEATHER'].fillna(value='uncertain', inplace=True)\n",
    "\n",
    "# Remove missing/uncertain values from ROADCOND\n",
    "df['ROADCOND'].replace(['Unknown', 'Other'], 'uncertain', inplace=True)\n",
    "df['ROADCOND'].fillna(value='uncertain', inplace=True)\n",
    "\n",
    "# Remove missing/uncertain values from LIGHTCOND\n",
    "df['LIGHTCOND'].replace(['Unknown', 'Other', 'Dark - Unknown Lighting'], 'uncertain', inplace=True)\n",
    "df['LIGHTCOND'].fillna(value='uncertain', inplace=True)\n",
    "\n",
    "# JUNTIONTYPE\n",
    "df['JUNCTIONTYPE'].replace(['Unknown'], 'uncertain', inplace=True)\n",
    "df['JUNCTIONTYPE'].fillna(value='uncertain', inplace=True)\n",
    "\n",
    "# ADDRTYPE\n",
    "df[df.ADDRTYPE.isna()] = 'NaN'\n",
    "df['ADDRTYPE'].replace(['NaN'], 'uncertain', inplace=True)\n",
    "\n",
    "# Drop NaN from target variable.\n",
    "df['SEVERITYCODE'].replace('NaN', np.nan, inplace=True)\n",
    "df = df.loc[~df.SEVERITYCODE.isna(), :]\n",
    "\n",
    "# GPS coords - substitute missing GPS coords with median value.\n",
    "df[['X', 'Y']].replace('NaN', np.nan, inplace=True)\n",
    "df['X'].fillna(np.median(df.X), inplace=True)\n",
    "df['Y'].fillna(np.median(df.Y), inplace=True)\n",
    "\n",
    "# Consider only complete years\n",
    "#Extract the hour\n",
    "df['YEAR'] = df['INCDTTM'].dt.year\n",
    "# remove faulty entries (ie not correct hour record)\n",
    "cond = df['YEAR'].isin([2020])\n",
    "df = df.loc[~cond, :].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the dataset without missing data or incomplete information.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SEVERITYCODE` target variable can take 2 values (1/2):\n",
    "- 1: **Property Damage**  \n",
    "- 2: **Injury**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations per target category exists?\n",
    "for category, counts in df.SEVERITYCODE.value_counts().items():\n",
    "    print(f\"There are {counts} elements in target value {category}.\")\n",
    "    \n",
    "# Print normalized count in each category of severity.\n",
    "norm_count = df.SEVERITYCODE.value_counts(normalize=True).to_dict()\n",
    "print(f\"The dataset is devided into{norm_count.get(1)*100: 0.2f}% Severity 1 and{norm_count.get(2)*100: 0.2f}% Severity 2 observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to observe, our dataset is quite imbalanced and this poses a challenge to classification algorithms. For now, we will consider the full dataset (i.e. records without missing or ambiguous information) and I will address later the problem of the imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of type 1 and type 2 severity events in the city of Seattle\n",
    "---\n",
    "\n",
    "Since we have GPS Data, the first step will be to cluster the city of Seattle into 10 different spatial clusters and identify those clusters. This information can also be relevant for the sponsors to target their actions to reduce accident severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the city of Seattle into 12 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initiate KMeans for 10 clusters (ie putative city areas)\n",
    "kmeans = KMeans(n_clusters = 12, init ='k-means++', random_state=32)\n",
    "\n",
    "# Get coords dataframe\n",
    "coords = df[['X', 'Y']]\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(coords) # Compute k-means clustering.\n",
    "\n",
    "# get the labels\n",
    "labels = kmeans.predict(coords) # Labels of each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# Set condition to access both types of severity independently\n",
    "cond = df['SEVERITYCODE'] == 1\n",
    "\n",
    "# Class 1\n",
    "ax[0].plot(df['X'][cond], df['Y'][cond], '.', ms=2, alpha=0.5, color='black')\n",
    "ax[0].set(ylabel='Latitude')\n",
    "ax[0].legend(labels=['Severity 1'])\n",
    "\n",
    "# Class 2\n",
    "ax[1].plot(df['X'][~cond], df['Y'][~cond], '.', ms=2, color='black', alpha=0.5)\n",
    "ax[1].legend(labels=['Severity 2'])\n",
    "\n",
    "# Both 1 and 2\n",
    "for event_type in [1, 2]:\n",
    "    ax[2].plot(df['X'][df['SEVERITYCODE'] == event_type], df['Y'][df['SEVERITYCODE'] == event_type], '.', ms=3, alpha=0.5)\n",
    "ax[2].legend(labels=['Severity 1', 'Severity 2'], loc='upper right')\n",
    "\n",
    "# Complete plot info\n",
    "for idx in range(3):\n",
    "    ax[idx].set(title='Seattle')\n",
    "    ax[idx].set(xlabel='Longitude')\n",
    "    ax[idx].tick_params('x', labelrotation=45)\n",
    "    \n",
    "    coords.plot.scatter(x = 'X', y = 'Y', c=labels, s=50, cmap='viridis', ax=ax[idx], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature: `SECTOR_ID` with labels from the clustering algorithm.\n",
    "df['SECTOR_ID'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total numbe of severity 1 and 2 events for each sector\n",
    "result = df.groupby(['SEVERITYCODE'])['SECTOR_ID'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "result.sort_values('NORM_COUNT', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='SECTOR_ID', y='NORM_COUNT', data=result, hue='SEVERITYCODE', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Sector 4 and 8 have a higher proportion of accident severity 2. This information can be used with one-hot encoder to create 2 more variables `SECTOR_4` and `SECTOR_8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECTOR_4'] = df['SECTOR_ID'].apply(lambda x: 1 if x == 4 else 0)\n",
    "df['SECTOR_8'] = df['SECTOR_ID'].apply(lambda x: 1 if x == 8 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When do events of specific severities happen more often?\n",
    "---\n",
    "In order to answer this question, we need to create new time features from the time feature in `INCDTTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the week day\n",
    "df['WEEKDAY'] = df['INCDTTM'].dt.weekday\n",
    "\n",
    "# Extract the hour\n",
    "df['HOUR'] = df['INCDTTM'].dt.hour\n",
    "\n",
    "# Extrack the month\n",
    "df['MONTH'] = df['INCDTTM'].dt.month\n",
    "\n",
    "# Extract the year\n",
    "df['YEAR'] = df['INCDTTM'].dt.year\n",
    "\n",
    "# Display top 3 columns\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of the total number of accidents for each category since 2004:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract monthly normalized count of events for each category\n",
    "result = df.groupby('SEVERITYCODE')['YEAR'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x='YEAR', y='NORM_COUNT', data=result, hue='SEVERITYCODE', ax=ax)\n",
    "\n",
    "ax.set(ylim=(0, 0.15), title='Yearly Accident variation for each Severity Category');\n",
    "ax.tick_params('x', labelrotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that accidents of type 2 tend to be more frequent since 2016. It is also interesting to notice an overall decrease in the number of accidents over the years. We can create a variable `SINCE_2016` that captures this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SINCE_2016'] = df['YEAR'].apply(lambda x: 1 if x >= 2016 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract monthly normalized count of events for each category\n",
    "result = df.groupby('SEVERITYCODE')['MONTH'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x='MONTH', y='NORM_COUNT', data=result, hue='SEVERITYCODE', ax=ax)\n",
    "\n",
    "month_labels = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "ax.set(ylim=(0, 0.15), xticklabels=month_labels, title='Monthly Accident proportion in each Severity Category');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that **JULY** and **AUGUST** have a (low, but still) slightly higher proportion of accidents in the category 2. We could use this information to create one additional one-hot encoded variable `SUMMER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SUMMER'] = df['MONTH'].apply(lambda x: 1 if x in [6, 7] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the proportion.\n",
    "df.groupby('SEVERITYCODE')['SUMMER'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about daily the variation in accident severity? Is there a particular time of the day with a higher proportion of accidents belonging to a specific severity type?\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estract normalized count of each severity category for each hour\n",
    "result = df.groupby('SEVERITYCODE')['HOUR'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(x='HOUR', y='NORM_COUNT', data=result, hue='SEVERITYCODE', ax=ax)\n",
    "\n",
    "ax.set(title='Hourly accident proportion in each severity Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that during the rush hour (7-8AM, 4-6PM) there is a higher proportion of accidents belonging to category 2, particularly during the rush hour (5-7PM). Let's capture this variation in another one-hot encoded variable `RUSH_HOUR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RUSH_HOUR'] = df['HOUR'].apply(lambda x: 1 if x in [7, 8, 16, 17, 18] else 0)\n",
    "\n",
    "df.groupby('SEVERITYCODE')['RUSH_HOUR'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the average human cost associated with each city sector?\n",
    "---\n",
    "In order to maximize the clustering information, it would be good to find a measure of human cost that could be used to describe each individual group (ie `SECTOR_ID`). One way to do this is to explore a measurement of the ratio between humans vs vehicles involved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single column that agregates all the non-motorized human involvement\n",
    "df['TOTAL_HUMAN'] = df[['PERSONCOUNT', 'PEDCYLCOUNT', 'PEDCOUNT']].sum(axis=1)\n",
    "df['TOTAL_HUMAN'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the relationship between total number of people involved and total number of vehicles involved?  \n",
    "---\n",
    "\n",
    "Before dwelling into the relation, let's have a look at the distribution of values within each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stats about severity and location\n",
    "df.groupby(['SEVERITYCODE'])[['VEHCOUNT', 'TOTAL_HUMAN']].agg([np.mean, np.min, np.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a higher number of people is associated with a severity of type 2, although the mean is only slightly lower. Let's look at the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Normalized log distribution of People involved in Accidents.')\n",
    "\n",
    "bins = np.linspace(0, 100, 20)\n",
    "\n",
    "ax[0].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==1], bins=bins, color='#1f77b4', label='Severity 1', log=True, density=True);\n",
    "ax[1].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==2], bins=bins, color='#ff7f0e', label='Severity 2', log=True, density=True);\n",
    "\n",
    "ax[2].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==2], log=True, bins=bins, color='#ff7f0e', label='Severity 2', density=True);\n",
    "ax[2].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==1], log=True, bins=bins, color='#1f77b4', label='Severity 1', density=True);\n",
    "\n",
    "\n",
    "for idx in range(3):\n",
    "    ax[idx].legend()\n",
    "    ax[idx].set(ylabel='normalized log count', xlabel='People')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot above (normalize to total count and in logarithmic scale) it is possible to observe that category 2 accidents involve frequently more humans than type 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VEHCOUNT'] = df.VEHCOUNT.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE', 'SECTOR_ID']).mean()[['TOTAL_HUMAN', 'VEHCOUNT']].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relation between total number of people involved vs vehicles.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('Relation between Vehicle Number and Humans involved.')\n",
    "\n",
    "sns.regplot(\n",
    "    x='TOTAL_HUMAN',\n",
    "    y='VEHCOUNT',\n",
    "    data=result,\n",
    "    order=1,\n",
    "    ax=ax[0]\n",
    ")\n",
    "ax[0].set(\n",
    "    ylabel='Mean Vehicle Number',\n",
    "    xlabel='Mean Human Number',\n",
    "    title='Linear Regression between vehicles and humans',\n",
    "    xlim=(2, 3.5)\n",
    ")\n",
    "\n",
    "sns.residplot(\n",
    "    x='TOTAL_HUMAN',\n",
    "    y='VEHCOUNT',\n",
    "    data=result,\n",
    "    order=1,\n",
    "    ax=ax[1]\n",
    ")\n",
    "ax[1].set(\n",
    "    title='Residual plot',\n",
    "    ylabel='Residuals from the model',\n",
    "    xlabel='',\n",
    "    ylim=(-0.15, 0.15),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the two variables correlated?\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, rvalue, pvalue, stderr = scipy.stats.linregress(result[['TOTAL_HUMAN', 'VEHCOUNT']])\n",
    "\n",
    "print(f\"The correlation coeff (person) is: {rvalue: 0.2f} and the p-value associated is {pvalue: 0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two variables are negatively correlated, meaning that there is a higher number of people associated with accidents involving fewer vehicles. \n",
    "\n",
    "## What is the ratio between total number of people involved and number of vehicles?\n",
    "---\n",
    "\n",
    "Let's now calculate the ratio between people involved and vehicles involved for each observation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add small value to VEH count to avoid ZeroDivisionError \n",
    "df[['VEHCOUNT']] = df[['VEHCOUNT']] + 0.001\n",
    "\n",
    "# Divide total_human by veh_count\n",
    "df['RATIO_HUMAN/VEH'] = df['TOTAL_HUMAN'].div(df['VEHCOUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the mean of the `RATIO_HUMAN/VEH` of each Cluster and for each severity category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_cost = df.groupby(['SEVERITYCODE', 'SECTOR_ID']).mean()['RATIO_HUMAN/VEH'].to_frame().reset_index()\n",
    "\n",
    "human_cost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Create a new feature called `SECTOR_HUMAN_COST` which stores the score of human cost for each sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_code_1 = human_cost[human_cost['SEVERITYCODE']==1][['SECTOR_ID', 'RATIO_HUMAN/VEH']]\n",
    "severity_code_1 = severity_code_1.to_dict().get('RATIO_HUMAN/VEH')\n",
    "\n",
    "severity_code_2 = human_cost[human_cost['SEVERITYCODE']==2][['SECTOR_ID', 'RATIO_HUMAN/VEH']].reset_index()\n",
    "severity_code_2 = severity_code_2.to_dict().get('RATIO_HUMAN/VEH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df['SECTOR_HUMAN_COST_a'] = df[df['SEVERITYCODE']==1]['SECTOR_ID'].replace(severity_code_1)\n",
    "df['SECTOR_HUMAN_COST_a'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECTOR_HUMAN_COST_b'] = df[df['SEVERITYCODE']==2]['SECTOR_ID'].replace(severity_code_2)\n",
    "df['SECTOR_HUMAN_COST_b'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECTOR_HUMAN_COST'] = df['SECTOR_HUMAN_COST_a'] + df['SECTOR_HUMAN_COST_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( figsize=(10, 5))\n",
    "sns.barplot(x='SEVERITYCODE', y='SECTOR_HUMAN_COST', data=df, hue='SECTOR_ID', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an uniform value to define each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_human_cost = human_cost['RATIO_HUMAN/VEH'][human_cost['SEVERITYCODE']==2].reset_index(drop=True)\\\n",
    "                 - human_cost['RATIO_HUMAN/VEH'][human_cost['SEVERITYCODE']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new variable \n",
    "df['SECTOR_HUMAN_COST'] = df['SECTOR_ID'].replace(net_human_cost.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results \n",
    "sns.barplot(\n",
    "    x='SECTOR_ID',\n",
    "    y='SECTOR_HUMAN_COST',\n",
    "    data=df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TOTAL_INTERV'] = df['TOTAL_HUMAN'] + df['VEHCOUNT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are specific types of road structures involved in specific types of events?\n",
    "---\n",
    "Among the data chosen to build the model, we can find information about types of collision addresses: Intersection and Block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['ADDRTYPE'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='ADDRTYPE', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    title='Accident severity and collision address type', \n",
    "    ylabel='Normalized count', \n",
    "    xlabel='Collision address type',\n",
    "    ylim=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Severity type 2 accidents happen in equal proportions in both Blocks and Intersections whereas type 1 accidents tend to happen more in Blocks.  \n",
    "\n",
    "## What about the road condition? How does road condition relate to accident severity?\n",
    "---\n",
    "There are several different attributes in terms of road condition, for instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Different states of road conditions', df.ROADCOND.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['ROADCOND'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='ROADCOND', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "ax.tick_params('x', labelrotation=45)\n",
    "ax.set(title='Road conditions and accident severity', ylabel='normalized count', xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are more accidents of category 2 during dry and wet conditions. This apparent paradox could be explained by overconfidence during dry and wet conditions that could induce reckless driving behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry = ['Dry']\n",
    "wet = ['Wet', 'Ice', \"Snow/Slush\", 'Sand/Mud/Dirt', 'Standing Water', 'Oil']\n",
    "\n",
    "\n",
    "for word in wet:\n",
    "    df['ROADCOND'] = df['ROADCOND'].replace(word, 'wet')\n",
    "for word in dry:\n",
    "    df['ROADCOND'] = df['ROADCOND'].replace(word, 'dry')\n",
    "    \n",
    "df.groupby('SEVERITYCODE').ROADCOND.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['SEVERITYCODE'])['LIGHTCOND'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['ROADCOND'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='ROADCOND', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "ax.tick_params('x', labelrotation=45)\n",
    "ax.set(title='Road conditions and accident severity', ylabel='normalized count', xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important component that can play a role in accident severity is the lighting conditions at the time of the accident.\n",
    "\n",
    "## Is there any relationship between accident severity and luminosity at the time of the accident?\n",
    "---\n",
    "Similarly to road condition, there are several different values associated with this feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SEVERITYCODE').LIGHTCOND.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['LIGHTCOND'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='LIGHTCOND', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "ax.tick_params('x', labelrotation=45)\n",
    "ax.set(title='Road conditions and accident severity', ylabel='normalized count', xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bin the values by good and bad visibility conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by similar condtions\n",
    "day_light = ['Daylight']\n",
    "low_light = ['Dark - Street Lights On', 'Dusk', 'Dawn', 'Dark - No Street Lights', 'Dark - Street Lights Off']\n",
    "\n",
    "for word in day_light:\n",
    "    df['LIGHTCOND'] = df['LIGHTCOND'].replace(word, 'light-good')\n",
    "for word in low_light:\n",
    "    df['LIGHTCOND'] = df['LIGHTCOND'].replace(word, 'light-bad')\n",
    "    \n",
    "df.LIGHTCOND.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['LIGHTCOND'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='LIGHTCOND', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "\n",
    "ax.set(ylim=(0, 1), title='Light conditions and accident severity', ylabel='normalized count', xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that good lightning conditions have a higher proportion of accidents of type 2 (i.e. injury-related). It could be explained by the higher number of people on the streets during daylight.\n",
    "\n",
    "\n",
    "\n",
    "While we have observed that Intersections have a higher proportion of type 1 accidents, We can explore the feature `JUNCTIONTYPE` to extract more insights about the properties of the accidents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junction Type\n",
    "df.groupby('SEVERITYCODE').JUNCTIONTYPE.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that type 1 Severity Accidents happen 50% of the times at Mid-Blocks whereas type 2 accidents happen with ~48% of the times at Intersection (intersection related). Since getting higher positive rate of type 2 accidents is our priority, I will use this information to create a one-hot encoded feature `INTERSECTION_RELATED` with this information:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['INTERSECTION_RELATED'] = df['JUNCTIONTYPE'].apply(lambda x: 1 if x == 'At Intersection (intersection related)' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How is the weather forecast related to accident severity?\n",
    "---\n",
    "The weather is also an important aspect of safe driving conditions and can have a strong impact on the type of accidents that can occur. In our dataset, weather can have the following values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WEATHER'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['WEATHER'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='WEATHER', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "\n",
    "ax.set(ylim=(0, 0.7), title='Weather and accident severity', ylabel='normalized count', xlabel='');\n",
    "ax.tick_params('x', labelrotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SEVERITYCODE').WEATHER.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of the accidents (>65%) happen during periods of Clear sky. Let's bin the different categories in: `sunny`, `cloudy` and `rainy-snow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eeather categories\n",
    "sunny = ['Clear', ]\n",
    "cloudy = ['Overcast', 'Fog/Smog/Smoke', 'Partly Cloudy', 'Blowing Sand/Dirt', 'Severe Crosswind', ]\n",
    "rainy_snow = ['Raining', 'Snowing', 'Sleet/Hail/Freezing Rain', ]\n",
    "\n",
    "for word in sunny:\n",
    "    df['WEATHER'] = df['WEATHER'].replace(word, 'sunny')\n",
    "for word in cloudy:\n",
    "    df['WEATHER'] = df['WEATHER'].replace(word, 'cloudy')\n",
    "for word in rainy_snow:\n",
    "    df['WEATHER'] = df['WEATHER'].replace(word, 'wet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['WEATHER'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='WEATHER', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "\n",
    "ax.set(ylim=(0, 0.7), title='Weather and accident severity', ylabel='normalized count', xlabel='weather forecast');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In agreement with the road condition variable, during sunny days (ie dry road) the amount of accidents of type 2 is higher compared to type one. It seems that wet road conditions are also associated with higher type 2 accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accidents at Crosswalks\n",
    "---\n",
    "Finally and to complete this basic EDA, let's have a look at the `CROSSWALKKEY` feature. This feature holds the key of the crossroad that is associated with an accident. Let's investigate whether accidents at crosswalks are associated with specific types of accidents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the variable in 1 where there is an event at crossroad (ie accident) or 0 elsewhere.\n",
    "df['CROSSWALKKEY'] = df['CROSSWALKKEY'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "df.groupby('SEVERITYCODE')['CROSSWALKKEY'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the majority of the accidents __do not__ take place at crossroads. However, severity 2 accidents tend to be associated in higher proportion to crossroads compared to severity 1 accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select and prepare data for modeling.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['SEVERITYCODE', 'ADDRTYPE', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'CROSSWALKKEY',\n",
    "         'SECTOR_4', 'SECTOR_8', 'WEEKDAY', 'MONTH', 'INTERSECTION_RELATED',\n",
    "         'RUSH_HOUR', 'SUMMER', 'TOTAL_HUMAN', 'SECTOR_HUMAN_COST']]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding of categorical data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset\n",
    "---\n",
    "#### Implementing a resampling method of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # https://imbalanced-learn.readthedocs.io/en/stable/api.html\n",
    "\n",
    "# Create resampling object \n",
    "rus = RandomUnderSampler(replacement=False, random_state=32, sampling_strategy='majority')\n",
    "\n",
    "# create target bool array to subset dataset\n",
    "target_cond = df.columns == 'SEVERITYCODE'\n",
    "\n",
    "# Undersample the data\n",
    "FEATURES, TARGET = rus.fit_sample(df.loc[:, ~target_cond], df.loc[:, target_cond])\n",
    "\n",
    "# Reassemble the dataset for preprocessing\n",
    "FEATURES['SEVERITYCODE'] = TARGET\n",
    "\n",
    "# Reassign variable to df\n",
    "df = FEATURES.copy()\n",
    "\n",
    "# Show the information about undersampled dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeling\"><a/>\n",
    "\n",
    "# Modeling\n",
    "---\n",
    "\n",
    "In this problem, I intend to build a binary classifier model based on previous observations. Thus, this problems represents a supervised learning problem and in order to tackle my classification problem I will use 3 types of machine learning algorithms:\n",
    "\n",
    "- Logistic Regression\n",
    "- Gradient Boosting Classifier\n",
    "- Random Forest ensemble\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Import preprocessing tools and model selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint, truncnorm, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features used to generate the model\n",
    "FEATURES = df.loc[:, df.columns != 'SEVERITYCODE']\n",
    "\n",
    "# What is intended to be predicted\n",
    "TARGET = df.loc[:, ~(df.columns != 'SEVERITYCODE')].replace({1: 0, 2: 1}).to_numpy().flatten()\n",
    "\n",
    "# Split the data into train/test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(FEATURES, TARGET, random_state=32, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data\n",
    "\n",
    "This is an important step for many algorithms. In order to implement cross validation, I will build a pipeline using a `sklearn.pipeline` object to ensure that there is no information leakage during cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy classifier\n",
    "The purpose of the dummy classifier is to assess how much of my model prediction is attributed to chance. For that I will implement and use a dummy classifier that chooses the most frequent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('dummy_clf', DummyClassifier(strategy='most_frequent', random_state=32)),\n",
    "]\n",
    "\n",
    "dummy_clf = Pipeline(steps)\n",
    "\n",
    "# Fit Data\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_dummy = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Model for Classification: Logistic Regression.\n",
    "\n",
    "Let's use grid search cross validation chained with a scaler object to find the best params to our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of steps for the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(random_state=32, max_iter=300))\n",
    "]\n",
    "\n",
    "# Assemble the pipeline for the logistic regression\n",
    "logreg = Pipeline(steps)\n",
    "\n",
    "# Implement stratified Cross Validation\n",
    "Kfold = KFold(n_splits=5)\n",
    "\n",
    "# Find the best parameters using gridsearch cross validation\n",
    "params_grid = [{ \n",
    "    'logreg__solver': ['liblinear'],\n",
    "    'logreg__penalty': ['l2'],\n",
    "    'logreg__C': [0.01, 0.1, 1, 10]\n",
    "},\n",
    "    {\n",
    "        'logreg__solver': ['saga'],\n",
    "        'logreg__penalty': ['l1', 'l2'],\n",
    "        'logreg__C': [0.01, 0.1, 1, 10]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Instantiate grid search cross validation object\n",
    "gs_logreg = GridSearchCV(logreg, params_grid, cv=Kfold);\n",
    "\n",
    "# Fit the model\n",
    "gs_logreg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best score for the GridSearch CrossValidation was{gs_logreg.best_score_: 0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best params\n",
    "gs_logreg_best_params = gs_logreg.best_params_\n",
    "\n",
    "print(gs_logreg_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GridSearch Cross Validation automatically refits the best params so we simply need to make predictions \n",
    "y_logreg = gs_logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Strategy:\n",
    "- Pipeline with scaler and estimator\n",
    "- RandomizedSearch coupled with stratified CrossValidation\n",
    "-  Use estimtor refit with best parameters for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Logistic regression model\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('forest', RandomForestClassifier(random_state=32, n_jobs=-1))\n",
    "]\n",
    "\n",
    "# Build pipeline for forest\n",
    "forest = Pipeline(steps)\n",
    "\n",
    "# Implement stratified Cross Validation\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Find the best parameters using gridsearch cross validation\n",
    "param_distributions = { \n",
    "    'forest__n_estimators': randint(100, 500),\n",
    "    'forest__max_depth': randint(1, 6),\n",
    "    'forest__max_features': truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n",
    "}\n",
    "\n",
    "# Instantiate grid search cross validation object\n",
    "gs_forest = RandomizedSearchCV(forest, param_distributions, cv=Kfold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "gs_forest.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_forest = gs_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n",
    "Strategy:\n",
    "- Pipeline with scaler and estimator\n",
    "- RandomizedSearch coupled with stratified CrossValidation\n",
    "-  Use estimtor refit with best parameters for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Logistic regression model\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gbrt', GradientBoostingClassifier(random_state=32))\n",
    "]\n",
    "\n",
    "# Build pipeline for forest\n",
    "gbrt = Pipeline(steps)\n",
    "\n",
    "# Implement stratified Cross Validation\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Find the best parameters using gridsearch cross validation\n",
    "param_distributions = { \n",
    "    'gbrt__n_estimators': randint(100, 300),\n",
    "    'gbrt__max_depth': randint(1, 5),\n",
    "    'gbrt__learning_rate': uniform(0, 1),\n",
    "}\n",
    "\n",
    "# Instantiate grid search cross validation object\n",
    "gs_gbrt = RandomizedSearchCV(gbrt, param_distributions, cv=Kfold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "gs_gbrt.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_gbrt = gs_gbrt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"><a/>\n",
    "\n",
    "# Evaluation\n",
    "    \n",
    "The objective of this project is to develop a model that predicts accurately accidents in the category 2, so that new policies can be developed to minimize those types of accidents. Thus, we want to have a model with a high True Positive Rate for Category 2 accidents.\n",
    "\n",
    "For this binary classification problem, I will use precision and recall measurements, associated F1 scores and I will have a look at the receiver operating characteristic (ROC) curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from sklearn.metrics import f1_score, classification_report, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Scores\n",
    "\n",
    "Let's look at the different F1 scores for each estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'   Logistic Regression: F1 Score on test set: {f1_score(y_test, y_logreg,): 0.2f}')\n",
    "print(f'Random Forest Ensemble: F1 Score on test set: {f1_score(y_test, y_forest,): 0.2f}')\n",
    "print(f'     Gradient Boosting: F1 Score on test set: {f1_score(y_test, y_gbrt,): 0.2f}')\n",
    "print(f'      Dummy Classifier: F1 Score on test set: {f1_score(y_test, y_dummy,): 0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "To better undertand the F1 scores above let's have a look at the `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Severity 1', 'Severity 2']\n",
    "\n",
    "print(f\"Logistic Regresion\\n--------------------\\n{classification_report(y_test, y_logreg, target_names=target_names)}\")\n",
    "print(\"Random Forest\\n--------------------\\n\", classification_report(y_test, y_forest, target_names=target_names))\n",
    "print(\"Gradient Boosting\\n---------------------\\n\", classification_report(y_test, y_gbrt, target_names=target_names))\n",
    "print(\"Dummy\\n---------------------\\n\", classification_report(y_test, y_dummy, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "By analysing the classification report, we can observe that the Random Forest Ensemble model provides the best recall (ie True Positive Rate) for severity 2 accidents which are the accidents that we intend to primarily target and reduce. Let's have a look at the confusion matrix to have a better visual understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, ax = plt.subplots(1, 4, figsize=(25, 5))\n",
    "\n",
    "for idx, (name, estimator) in enumerate({'Logistic Regression': gs_logreg.best_estimator_,\n",
    "                                         'Random Forest': gs_forest.best_estimator_,\n",
    "                                         'Gradient Boosting': gs_gbrt.best_estimator_,\n",
    "                                         'Dummy Classifier': dummy_clf,\n",
    "                                        }.items()):\n",
    "    plot_confusion_matrix(estimator, X_test, y_test, ax=ax[idx], normalize='true', display_labels=target_names, )\n",
    "    \n",
    "    ax[idx].set(title=f'{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/recall curves\n",
    "\n",
    "Another way to evaluate the best model is to have a look at the precision/recall curves. In a classification task, we can look at the tradeof of maximizing:\n",
    "- recall (reduction of false negatives > accidents predicted to be of severity 1 but that are instead of severity 2)\n",
    "or\n",
    "- precision (reduction of false positive > accidents predicted to be of severity 2 but that are of severity 1)\n",
    "\n",
    "In your example, we thrive to reduce false negatives (reduce the so called type 2 error) as the human cost associated with a false negative (failing to predict a type 2 severity accident) is higher than a false positive (prediction of a type 2 accident that is indeed false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Recall Curves\n",
    "fig, ax = plt.subplots(1, 4, figsize=(25, 5),)\n",
    "for idx, (name, estimator) in enumerate({'Logistic Regression': gs_logreg.best_estimator_,\n",
    "                                         'Random Forest': gs_forest.best_estimator_,\n",
    "                                         'Gradient Boosting': gs_gbrt.best_estimator_,\n",
    "                                         'Dummy Classifier': dummy_clf,\n",
    "                                        }.items()):\n",
    "    plot_precision_recall_curve(estimator, X_test, y_test, ax=ax[idx])\n",
    "    ax[idx].set(title=f'{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Curves\n",
    "\n",
    "Another way to evaluate the performance of our classifier is to look at the receiver operating curves. An ideal ROC curve should have a high area under the curve (AUC) and should be closer to the upper left corner (High True Positive Rate: Most accidents correctly classifier in their categories + Low False Positive Rate: Small number of accidents mispredicted).\n",
    "\n",
    "As it is possible to observe, both Random Forest and Gradient Boosting algorithms have the higher AUC, however based on our previous analysis, the best estimator for our problem is the Random Forest. As expected, the dummy classifier has AUC of 0.50 meaning that predictions are completely random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(25, 5))\n",
    "for idx, (name, estimator) in enumerate({'Logistic Regression': gs_logreg.best_estimator_,\n",
    "                                         'Random Forest': gs_forest.best_estimator_,\n",
    "                                         'Gradient Boosting': gs_gbrt.best_estimator_,\n",
    "                                         'Dummy Classifier': dummy_clf,\n",
    "                                        }.items()):\n",
    "    plot_roc_curve(estimator, X_test, y_test, ax=ax[idx])\n",
    "    ax[idx].plot([0, 1], [0, 1], 'k--')\n",
    "    ax[idx].set(title=f'{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"><a/>\n",
    "\n",
    "# Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mode, I manage to develop a model that is able to correctly classify severity 2 accidents (i.e. injury-related) with a F1 score of 0.69. Particularly, this model predicted correctly accidents of type 2 ~70% of the times and thus it is suitable to guide decision making processes intending at reducing this type of accident. As the model and respective pe designed to reduce the number of accidents and improve the road safety and overall city safety.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
