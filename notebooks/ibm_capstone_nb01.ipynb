{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving public policy by predicting car accident severity using machine learning\n",
    "\n",
    "This notebook will be used to develop the capstone project of the IBM Certification in Data Science.\n",
    "\n",
    "The purpose of this project is to predict accident using attributes collected by the Seattle Polive Department and recorded by Traffic Records.\n",
    "\n",
    "\n",
    "Original dataset: [link_dataset](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv)  \n",
    "Metadata: [link_metadata](https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Metadata.pdf)\n",
    "\n",
    "In order to get the most out of this dataset, I followed the \n",
    "\n",
    "\n",
    "---\n",
    "## Table of Contents\n",
    "\n",
    "### 1 - [Introduction/Business Problem](#business-und)\n",
    "\n",
    "### 2 - [Data understanding and Data Preparation](#data-und-data-prep)\n",
    "\n",
    "### 3 - [Modeling](#modeling)\n",
    "\n",
    "### 4 - [Evaluation](#evaluation)\n",
    "\n",
    "### 5 - [Conclusion](#conclusion)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import necessary packages\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "# Ploting params\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "# axes.titlesize : 24\n",
    "# axes.labelsize : 20\n",
    "# lines.linewidth : 3\n",
    "# lines.markersize : 10\n",
    "# xtick.labelsize : 16\n",
    "# ytick.labelsize : 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"business-und\"><a/>\n",
    "\n",
    "# 1 - Introduction/Business Problem\n",
    "---- \n",
    "\n",
    "\n",
    "**Disclaimer:** _This project is part of the Coursera Capstone to complete the Data Science Certification. All project ideas are entirely fictitious and serve only the purpose of developing a data science project._\n",
    "\n",
    "#### Predicting accident severity in US cities based on specific metrics can be a powerful tool to drive public policy and reduce overall accident rate. In Seattle – Washington, the Department of Transport/Traffic Management Division,  has been collecting data since 2004 about collisions in the metropolitan area with the objective of creating a complete database that represents the overall road accidents involving collisions in this city. The City Council of Seattle has the responsibility of approving the city's budget, and develops laws and policies intended to promote safety of Seattle's residents  . During every fiscal year, the city council discusses policies to improve road safety in Seattle by reducing the number of human injuries involved in those accidents. \n",
    "#### In this data science project, I proposed to develop a model that can distinguish accidents resulting in human injuries from accidents resulting in property damage-only. This prediction will be based on widely available metrics provided by the Department of transport and it can help to identify which factors may increase the risk for injury-related accidents and help develop actions to reduce those. The successful outcome of this project would be a model that can predict with accuracy accidents associated with human costs (i.e. high true positive rate), so that actions can be developed to minimize those costs. Examples of actions could include target advertising for road safety, improve road design, increase police deployment to secure roads, increase fines for reckless driving, among others. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-und-data-prep\"><a/>\n",
    "\n",
    "# 2 - Data understanding and Data Preparation\n",
    "---\n",
    "    \n",
    "#### To develop my data project, I will use data about collisions provided by Seattle Police Department and recorded by Traffic Records . This dataset includes all types of collisions happening at intersection or mid-block of a road segment since 2004 and contains information about many important factors such as road condition and lightning conditions, weather, segment of the road involved (among other) and associated with each accident there is a variable that represents the outcome severity with 2 values: type 1 – property damage-only and type 2 – Injury-related. By using this information, I will develop a classification model aiming at predicting the severity outcome of the accident, with particular emphasis at predicting type 2 accidents.  I will focus on optimizing the model to get a high true positive rate of detection of type 2 accidents and the results obtained from this model could guide actions to decrease the occurrence of these accidents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the location of the data\n",
    "data_url = 'https://github.com/joseferncruz/coursera-capstone/raw/master/datasets/collisions_data.csv'\n",
    "\n",
    "# load the data into a dataframe\n",
    "df_raw = pd.read_csv(\n",
    "    data_url,\n",
    "    parse_dates=['INCDTTM', 'INCDATE'], # Parse dates to datetime objects \n",
    "    usecols=[\n",
    "        'SEVERITYCODE', 'X', 'Y', 'ADDRTYPE', 'PERSONCOUNT', 'PEDCOUNT',\n",
    "        'PEDCYLCOUNT', 'VEHCOUNT', 'INCDTTM', 'JUNCTIONTYPE', 'WEATHER',\n",
    "        'ROADCOND', 'LIGHTCOND', 'CROSSWALKKEY', 'INCDATE',\n",
    "    ],\n",
    "    low_memory=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by exploring the features and characteristics of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This dataset has {df_raw.shape[0]} rows and {df_raw.shape[1]} columns, including one target column.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some information about the datatypes and number of entries associated with the full raw dataset\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By exploring the metadata, it is possible to notice that some columns have large quantities of missing data or lack information (Not Enough Information or NEI). Also our model should be able to predict accident severity based on features (ie information) than can be measured in real time or within hour range (eg weather, road conditiong, etc).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not used in this case study\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to solve this classification problem, I will use the following features from the original dataset:\n",
    "\n",
    "\n",
    "|Attributes| Description|  \n",
    "|---:|:----|  \n",
    "|SEVERITYCODE|A code that corresponds to the severity of the collision|  \n",
    "| X | GPS Latitude coordinate |\n",
    "| Y | GPS Longitude coordinate | \n",
    "|ADDRTYPE|Collision address type|\n",
    "|PERSONCOUNT|The total number of people involved in the collision|\n",
    "|PEDCOUNT|The number of pedestrians involved in the collision |\n",
    "|PEDCYLCOUNT| The number of bicycles involved in the collision. |\n",
    "|VEHCOUNT|The number of vehicles involved inthe collision|\n",
    "|INCDTTM|The date and time of the incident.|\n",
    "| JUNCTIONTYPE| Category of junction at which collision took place |\n",
    "|WEATHER|A description of the weather conditions during the time of the collision|\n",
    "|ROADCOND|The condition of the road during the collision|\n",
    "|LIGHTCOND|The light conditions during the collision|\n",
    "|CROSSWALKKEY| A key for the crosswalk at which the collision occurred |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is to remove missing/unknown information from columns in order to reduce the uncertainty around certain features. In this project I will consider only observations that have complete, unambigous information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries with missing information\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove missing/uncertain values from WEATHER\n",
    "cond = df['WEATHER'].isin(['Unknown', 'Other'])\n",
    "df = df.loc[~cond, :].copy()\n",
    "\n",
    "# Remove missing/uncertain values from ROADCOND\n",
    "cond = df['ROADCOND'].isin(['Unknown', 'Other'])\n",
    "df = df.loc[~cond, :].copy()\n",
    "\n",
    "# Remove missing/uncertain values from LIGHTCOND\n",
    "cond = df['LIGHTCOND'].isin(['Unknown', 'Other', 'Dark - Unknown Lighting'])\n",
    "df = df.loc[~cond, :].copy()\n",
    "\n",
    "# JUNTIONTYPE\n",
    "cond = df['JUNCTIONTYPE'].isin(['Unknown'])\n",
    "df = df.loc[~cond, :]\n",
    "\n",
    "# Remove alleys from address type due to low samples\n",
    "df = df.loc[~df['ADDRTYPE'].isin(['Alley']), :].copy()\n",
    "\n",
    "# remove events with missing hour \n",
    "# Extract the hour\n",
    "df['HOUR'] = df['INCDTTM'].dt.hour\n",
    "# remove faulty entries (ie not correct hour record)\n",
    "cond = df['HOUR'].isin([0])\n",
    "df = df.loc[~cond, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the dataset without missing data or incomplete information.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SEVERITYCODE` target variable can take 2 values (1/2):\n",
    "- 1: **Property Damage**  \n",
    "- 2: **Injury**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations per target category exists?\n",
    "for category, counts in df.SEVERITYCODE.value_counts().items():\n",
    "    print(f\"There are {counts} elements in target value {category}.\")\n",
    "    \n",
    "# Print normalized count in each category of severity.\n",
    "norm_count = df.SEVERITYCODE.value_counts(normalize=True).to_dict()\n",
    "print(f\"The dataset is devided into{norm_count.get(1)*100: 0.2f}% Severity 1 and{norm_count.get(2)*100: 0.2f}% Severity 2 observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As it is possible to observe, our dataset is quite imbalanced and this poses a challenge to classification algorithms. For now, we will consider the full dataset (ie records without missing or ambigous information) and I will address later the problem of the imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location of type 1 and type 2 severity events in the city of Seattle\n",
    "\n",
    "#### Since we have GPS Data, the first step will be to cluster the city of seattle into 10 different spatial clusters and identify those clusters. This information can also be relevant for the sponsors to target their actions to reduce accident severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the city of Seattle into 12 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initiate KMeans for 10 clusters (ie putative city areas)\n",
    "kmeans = KMeans(n_clusters = 12, init ='k-means++', random_state=32)\n",
    "\n",
    "# Get coords dataframe\n",
    "coords = df[['X', 'Y']]\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(coords) # Compute k-means clustering.\n",
    "\n",
    "# get the labels\n",
    "labels = kmeans.predict(coords) # Labels of each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# Set condition to access both types of severity independently\n",
    "cond = df['SEVERITYCODE'] == 1\n",
    "\n",
    "# Class 1\n",
    "ax[0].plot(df['X'][cond], df['Y'][cond], '.', ms=2, alpha=0.5, color='black')\n",
    "ax[0].set(ylabel='Latitude')\n",
    "ax[0].legend(labels=['Severity 1'])\n",
    "\n",
    "# Class 2\n",
    "ax[1].plot(df['X'][~cond], df['Y'][~cond], '.', ms=2, color='black', alpha=0.5)\n",
    "ax[1].legend(labels=['Severity 2'])\n",
    "\n",
    "# Both 1 and 2\n",
    "for event_type in [1, 2]:\n",
    "    ax[2].plot(df['X'][df['SEVERITYCODE'] == event_type], df['Y'][df['SEVERITYCODE'] == event_type], '.', ms=3, alpha=0.5)\n",
    "ax[2].legend(labels=['Severity 1', 'Severity 2'], loc='upper right')\n",
    "\n",
    "# Complete plot info\n",
    "for idx in range(3):\n",
    "    ax[idx].set(title='Seattle')\n",
    "    ax[idx].set(xlabel='Longitude')\n",
    "    ax[idx].tick_params('x', labelrotation=45)\n",
    "    \n",
    "    coords.plot.scatter(x = 'X', y = 'Y', c=labels, s=50, cmap='viridis', ax=ax[idx], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature: `SECTOR_ID` with labels from the clustering algorithm.\n",
    "df['SECTOR_ID'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total numbe of severity 1 and 2 events for each sector\n",
    "result = df.groupby(['SEVERITYCODE'])['SECTOR_ID'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "result.sort_values('NORM_COUNT', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='SECTOR_ID', y='NORM_COUNT', data=result, hue='SEVERITYCODE', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that Sector 5 and 7 have higher proportion of accident severity 2. This information can be used with one-hot encoder to create 2 more variables `SECTOR_5` and `SECTOR_7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECTOR_5'] = df['SECTOR_ID'].apply(lambda x: 1 if x == 5 else 0)\n",
    "df['SECTOR_7'] = df['SECTOR_ID'].apply(lambda x: 1 if x == 5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When do events of specific severities happen more often?\n",
    "---\n",
    "#### In order to answer this question, we need to create new time features from the time feature in `INCDTTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the week day\n",
    "df['WEEKDAY'] = df['INCDTTM'].dt.weekday\n",
    "\n",
    "# Extract the hour\n",
    "df['HOUR'] = df['INCDTTM'].dt.hour\n",
    "\n",
    "# Extrack the month\n",
    "df['MONTH'] = df['INCDTTM'].dt.month\n",
    "\n",
    "# Extract the year\n",
    "df['YEAR'] = df['INCDTTM'].dt.year\n",
    "\n",
    "# Display top 3 columns\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract monthly normalized count of events for each category\n",
    "result = df.groupby('SEVERITYCODE')['MONTH'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x='MONTH', y='NORM_COUNT', data=result, hue='SEVERITYCODE', ax=ax)\n",
    "\n",
    "month_labels = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "ax.set(ylim=(0, 0.15), xticklabels=month_labels, title='Monthly Accident proportion in each Severity Category');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that **JULY** and **AUGUST** have a (low, but still) slightly higher proportion of accidents in the category 2. We could used this information to create one additional one-hot encoded variable `SUMMER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SUMMER'] = df['MONTH'].apply(lambda x: 1 if x in [6, 7] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the proportion.\n",
    "df.groupby('SEVERITYCODE')['SUMMER'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about daily modulation in accident severity? Is there a particular time of the day with higher proportion of accidents belonging to a specific severity?\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estract normalized count of each severity category for each hour\n",
    "result = df.groupby('SEVERITYCODE')['HOUR'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(x='HOUR', y='NORM_COUNT', data=result, hue='SEVERITYCODE', ax=ax)\n",
    "\n",
    "ax.set(title='Hourly accident proportion in each severity Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that during the afternoon there higher a higher proportion of accidents belonging to category 2, particularly during the rush rour (5-7PM). Let's capture this variation in another one-hot encoded variable `AFTERNOON`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AFTERNOON'] = df['HOUR'].apply(lambda x: 1 if x in [14, 15, 16, 17, 18] else 0)\n",
    "\n",
    "df.groupby('SEVERITYCODE')['AFTERNOON'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the average human cost associated with each city sector?\n",
    "---\n",
    "#### In order to maximize the clustering information, it would be good to find a measure of human cost that could be used to describe each individual group (ie `SECTOR_ID`). One way to do this is to explore a measurement of the ratio between of humans vs vehicles involved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single column that agregates all the non-motorized human involvement\n",
    "df['TOTAL_HUMAN'] = df[['PERSONCOUNT', 'PEDCYLCOUNT', 'PEDCOUNT']].sum(axis=1)\n",
    "df['TOTAL_HUMAN'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the relationship between total number of people involved and total number of vehicles involved?  \n",
    "\n",
    "#### Before dwelling into to the relation, let's have a look at the distribution of values within each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stats about severity and location\n",
    "df.groupby(['SEVERITYCODE'])[['VEHCOUNT', 'TOTAL_HUMAN']].agg([np.mean, np.min, np.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that a higher number of people is associated with a severity of type 2, alghouth the mean is only slightly lower. Let's look at the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('Distribution of People involved in Accidents.')\n",
    "\n",
    "bins = 10 \n",
    "\n",
    "ax[0].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==1], log=True, bins=bins, color='#1f77b4', label='Severity 1', normed=True);\n",
    "ax[1].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==2], log=True, bins=bins, color='#ff7f0e', label='Severity 2', normed=True);\n",
    "\n",
    "ax[2].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==1], log=True, bins=bins, color='#1f77b4', label='Severity 1', normed=True);\n",
    "ax[2].hist(df['TOTAL_HUMAN'][df['SEVERITYCODE']==2], log=True, bins=bins, color='#ff7f0e', label='Severity 2', normed=True);\n",
    "\n",
    "for idx in range(3):\n",
    "    ax[idx].legend()\n",
    "    ax[idx].set(ylabel='count (log)', xlabel='People')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE', 'SECTOR_ID']).mean()[['TOTAL_HUMAN', 'VEHCOUNT']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relation between total number of people involved vs vehicles.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Relation between Vehicle Number and Humans involved.')\n",
    "\n",
    "sns.regplot(\n",
    "    x='TOTAL_HUMAN',\n",
    "    y='VEHCOUNT',\n",
    "    data=result,\n",
    "    order=1,\n",
    "    ax=ax[0]\n",
    ")\n",
    "ax[0].set(\n",
    "    ylabel='Mean Vehicle Number',\n",
    "    xlabel='Mean Human Number',\n",
    "    title='Linear Regression between vehicles and humans')\n",
    "\n",
    "sns.residplot(\n",
    "    x='TOTAL_HUMAN',\n",
    "    y='VEHCOUNT',\n",
    "    data=result,\n",
    "    order=1,\n",
    "    ax=ax[1]\n",
    ")\n",
    "ax[1].set(\n",
    "    title='Residual plot',\n",
    "    ylabel='Residuals from the model',\n",
    "    xlabel='',\n",
    "    ylim=(-0.15, 0.15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the two variables correlated?\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, rvalue, pvalue, stderr = scipy.stats.linregress(result[['TOTAL_HUMAN', 'VEHCOUNT']])\n",
    "\n",
    "print(f\"The correlation coeff (person) is: {rvalue: 0.2f} and the p-value associated is {pvalue: 0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The two variables are negatively correlated, meaning that there is a higher number of people associated with accidents involving fewer vehicles. \n",
    "\n",
    "## What is the ratio between total number of people involved and number of vehicles?\n",
    "---\n",
    "\n",
    "#### Let's now calculate the ratio between people involved and vehicles involved for each observation in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add small value to VEH count to avoid ZeroDivisionError \n",
    "df[['VEHCOUNT']] = df[['VEHCOUNT']] + 0.001\n",
    "\n",
    "# Divide total_human by veh_count\n",
    "df['RATIO_HUMAN/VEH'] = df['TOTAL_HUMAN'].div(df['VEHCOUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's check the mean of the `RATIO_HUMAN/VEH` of each Cluster and for each severity catergory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_cost = df.groupby(['SEVERITYCODE', 'SECTOR_ID']).mean()['RATIO_HUMAN/VEH'].to_frame().reset_index()\n",
    "\n",
    "human_cost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Create a new feature called `SECTOR_HUMAN_COST` which stores the score of human cost for each sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_code_1 = human_cost[human_cost['SEVERITYCODE']==1][['SECTOR_ID', 'RATIO_HUMAN/VEH']]\n",
    "severity_code_1 = severity_code_1.to_dict().get('RATIO_HUMAN/VEH')\n",
    "\n",
    "severity_code_2 = human_cost[human_cost['SEVERITYCODE']==2][['SECTOR_ID', 'RATIO_HUMAN/VEH']].reset_index()\n",
    "severity_code_2 = severity_code_2.to_dict().get('RATIO_HUMAN/VEH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df['SECTOR_HUMAN_COST_a'] = df[df['SEVERITYCODE']==1]['SECTOR_ID'].replace(severity_code_1)\n",
    "df['SECTOR_HUMAN_COST_a'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECTOR_HUMAN_COST_b'] = df[df['SEVERITYCODE']==2]['SECTOR_ID'].replace(severity_code_2)\n",
    "df['SECTOR_HUMAN_COST_b'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECTOR_HUMAN_COST'] = df['SECTOR_HUMAN_COST_a'] + df['SECTOR_HUMAN_COST_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='SEVERITYCODE', y='SECTOR_HUMAN_COST', data=df, hue='SECTOR_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a unifrom value to define each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_human_cost = human_cost['RATIO_HUMAN/VEH'][human_cost['SEVERITYCODE']==2].reset_index(drop=True)\\\n",
    "                 - human_cost['RATIO_HUMAN/VEH'][human_cost['SEVERITYCODE']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new variable \n",
    "df['SECTOR_HUMAN_COST'] = df['SECTOR_ID'].replace(net_human_cost.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results \n",
    "sns.barplot(\n",
    "    x='SECTOR_ID',\n",
    "    y='SECTOR_HUMAN_COST',\n",
    "    data=df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TOTAL_INTERV'] = df['TOTAL_HUMAN'] + df['VEHCOUNT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are specific types of road structures involved in specific types of events?\n",
    "---\n",
    "#### Among the data choosen to build the model, we can find information about types of collision addresses: Intersection and Block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['ADDRTYPE'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='ADDRTYPE', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    title='Accident severity and collision address type', \n",
    "    ylabel='Normalized count', \n",
    "    xlabel='Collision address type',\n",
    "    ylim=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that Severity type 2 accidents happen in equally proportions in both Blocks and Intersections whereas type 1 accidents tend to happen more in Blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the road condition? How does road condition relate to accident severity?\n",
    "---\n",
    "There are several different attributes in terms of road condition, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Different states of road conditions', df.ROADCOND.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['ROADCOND'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='ROADCOND', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "ax.tick_params('x', labelrotation=45)\n",
    "ax.set(title='Road conditions and accident severity', ylabel='normalized count', xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that there are no striking differences between road conditions and the different types of accident severity. In order to decrease the number of different categories, we will bin some of the variables with similar conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry = ['Dry']\n",
    "wet = ['Wet', 'Ice', \"Snow/Slush\", 'Sand/Mud/Dirt', 'Standing Water', 'Oil']\n",
    "\n",
    "\n",
    "for word in wet:\n",
    "    df['ROADCOND'] = df['ROADCOND'].replace(word, 'wet')\n",
    "for word in dry:\n",
    "    df['ROADCOND'] = df['ROADCOND'].replace(word, 'dry')\n",
    "    \n",
    "df.groupby('SEVERITYCODE').ROADCOND.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['SEVERITYCODE'])['LIGHTCOND'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another important component that can play a role in accident severity is the lighting conditiong at the time of the accident. \n",
    "## Is there any relationship between accident severity and luminosity at the time of the accident?\n",
    "---\n",
    "Similarly to road condition, there are several different values associated with this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SEVERITYCODE').LIGHTCOND.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's bin the values by good and bad visibility condiitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by similar condtions\n",
    "day_light = ['Daylight']\n",
    "low_light = ['Dark - Street Lights On', 'Dusk', 'Dawn', 'Dark - No Street Lights', 'Dark - Street Lights Off']\n",
    "\n",
    "for word in day_light:\n",
    "    df['LIGHTCOND'] = df['LIGHTCOND'].replace(word, 'light-good')\n",
    "for word in low_light:\n",
    "    df['LIGHTCOND'] = df['LIGHTCOND'].replace(word, 'light-bad')\n",
    "    \n",
    "df.LIGHTCOND.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['LIGHTCOND'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='LIGHTCOND', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "\n",
    "ax.set(ylim=(0, 1), title='Light conditions and accident severity', ylabel='normalized count', xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that good lightning conditions have slightly higher proportion of accidents of type 2 (ie injury-related). It could be explained for instance buy driver over confidence during a seemingly safer road period (ie daylight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### While we have observed that Insersections have higher proportion of type 1 accidents, We can explore the feature `JUNCTIONTYPE` to extract more insights about the properties of the accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junction Type\n",
    "df.groupby('SEVERITYCODE').JUNCTIONTYPE.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that type 1 Severity Accidentds happen 50% of the times at Mid-Blocks whereas type 2 accidents happen with ~48% of the times at Intersection (intersection related). Since getting higher positive rate of type 2 accidents is our priority, I will use this information to create a one-hot encoded feature `INTERSECTION_RELATED` with this informatinon: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['INTERSECTION_RELATED'] = df['JUNCTIONTYPE'].apply(lambda x: 1 if x == 'At Intersection (intersection related)' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is weather forecast related to accident severity type?\n",
    "---\n",
    "The weather is also an important aspect of safe driving conditions and can have a strong impact on the type of accidents that can place. In our dataset, weather can have the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WEATHER'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['WEATHER'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='WEATHER', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "\n",
    "ax.set(ylim=(0, 0.7), title='Weather and accident severity', ylabel='normalized count', xlabel='');\n",
    "ax.tick_params('x', labelrotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('SEVERITYCODE').WEATHER.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The vast majority of the accidents (>65%) happen during periods of Clear sky. Let's bin the different categories in: `sunny`, `cloudy` and `rainy-snow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eeather categories\n",
    "sunny = ['Clear', ]\n",
    "cloudy = ['Overcast', 'Fog/Smog/Smoke', 'Partly Cloudy', 'Blowing Sand/Dirt', 'Severe Crosswind', ]\n",
    "rainy_snow = ['Raining', 'Snowing', 'Sleet/Hail/Freezing Rain', ]\n",
    "\n",
    "for word in sunny:\n",
    "    df['WEATHER'] = df['WEATHER'].replace(word, 'sunny')\n",
    "for word in cloudy:\n",
    "    df['WEATHER'] = df['WEATHER'].replace(word, 'cloudy')\n",
    "for word in rainy_snow:\n",
    "    df['WEATHER'] = df['WEATHER'].replace(word, 'wet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupby(['SEVERITYCODE'])['WEATHER'].value_counts(normalize=True).to_frame('NORM_COUNT').reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(\n",
    "    data=result, \n",
    "    x='WEATHER', \n",
    "    y='NORM_COUNT', \n",
    "    hue='SEVERITYCODE',\n",
    ")\n",
    "\n",
    "ax.set(ylim=(0, 0.7), title='Weather and accident severity', ylabel='normalized count', xlabel='weather forecast');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accidents at Crosswalks\n",
    "---\n",
    "#### Finaly and to complete this basic EDA, let's have a look at the `CROSSWALKKEY` feature. This feature hold the key of the crossroad that is associated with an accident. Let's investigate whether accidents at crosswalks are associated with specific types of accidents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the variable in 1 where there is an event at crossroad (ie accident) or 0 elsewhere.\n",
    "df['CROSSWALKKEY'] = df['CROSSWALKKEY'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "df.groupby('SEVERITYCODE')['CROSSWALKKEY'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that the majority of the accidents do not take place at crossroads. However, severity 2 accidents tend to associated in higher proportion to crossroads compared to severity 1 accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select and prepare data for modeling.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['SEVERITYCODE', 'ADDRTYPE', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'CROSSWALKKEY',\n",
    "         'SECTOR_5', 'SECTOR_7', 'WEEKDAY', 'MONTH', 'INTERSECTION_RELATED',\n",
    "         'AFTERNOON', 'SUMMER', 'TOTAL_HUMAN', 'SECTOR_HUMAN_COST']]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding of categorical data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset\n",
    "---\n",
    "#### Implementing a resampling method of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # https://imbalanced-learn.readthedocs.io/en/stable/api.html\n",
    "\n",
    "# Create resampling object \n",
    "rus = RandomUnderSampler(replacement=False, random_state=32, sampling_strategy='majority')\n",
    "\n",
    "# create target bool array to subset dataset\n",
    "target_cond = df.columns == 'SEVERITYCODE'\n",
    "\n",
    "# Undersample the data\n",
    "FEATURES, TARGET = rus.fit_sample(df.loc[:, ~target_cond], df.loc[:, target_cond])\n",
    "\n",
    "# Reassemble the dataset for preprocessing\n",
    "FEATURES['SEVERITYCODE'] = TARGET\n",
    "\n",
    "# Reassign variable to df\n",
    "df = FEATURES.copy()\n",
    "\n",
    "# Show the information about undersampled dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeling\"><a/>\n",
    "\n",
    "# Modeling\n",
    "---\n",
    "\n",
    "In this problem, I intend to build a binary classifier model based on previous observations. Thus, this problems represents a supervised learning problem and in order to tackle my classification problem I will use 3 types of machine learning algorithms:\n",
    "\n",
    "- Logistic Regression\n",
    "- Gradient Boosting Classifier\n",
    "- Random Forest ensemble\n",
    "\n",
    "[Descriptin about the method and implementatio]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Import preprocessing tools and model selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, KFold, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features used to generate the model\n",
    "FEATURES = df.loc[:, df.columns != 'SEVERITYCODE']\n",
    "\n",
    "# What is intended to be predicted\n",
    "TARGET = df.loc[:, ~(df.columns != 'SEVERITYCODE')].replace({1: 0, 2: 1}).to_numpy().flatten()\n",
    "\n",
    "# Split the data into train/test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(FEATURES, TARGET, random_state=32, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data\n",
    "#### This step is important for the Logistic Regression Model although it would be unecessary for Decision Tree based models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a standard scaler on the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale train data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale test data with scaler object fit with train data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy classifier\n",
    "#### The purpose of the dummy classifier is to assess how much of my model predicts are attributed to chance. For that I will implement and use a dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create the dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy='uniform', random_state=32)\n",
    "\n",
    "# Fit Data\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_dummy = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Model for Classification: Logistic Regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use grid search cross validation to find the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Logistic regression model\n",
    "logreg = LogisticRegression(random_state=32, max_iter=200)\n",
    "\n",
    "# Implement stratified Cross Validation\n",
    "Kfold = KFold(n_splits=5)\n",
    "\n",
    "# Find the best parameters using gridsearch cross validation\n",
    "params_grid = [{ \n",
    "    'solver': ['liblinear'],'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10, 100]\n",
    "},\n",
    "    {\n",
    "        'solver': ['saga'], 'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Instantiate grid search cross validation object\n",
    "gs_logreg = GridSearchCV(logreg, params_grid, cv=Kfold);\n",
    "\n",
    "# Fit the model\n",
    "gs_logreg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best score for the GridSearch CrossValidation was{gs_logreg.best_score_: 0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best params\n",
    "gs_logreg_best_params = gs_logreg.best_params_\n",
    "\n",
    "print(gs_logreg_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Logistic regression model\n",
    "logreg = LogisticRegression(\n",
    "    penalty=gs_logreg_best_params.get('penalty'),\n",
    "    C=gs_logreg_best_params.get('C'),\n",
    "    solver=gs_logreg_best_params.get('solver'),\n",
    ")\n",
    "\n",
    "# Fit the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict using logreg\n",
    "y_logreg = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Logistic regression model\n",
    "forest = RandomForestClassifier(random_state=32, n_jobs=-1)\n",
    "\n",
    "# Implement stratified Cross Validation\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Find the best parameters using gridsearch cross validation\n",
    "params_grid = { \n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [4, 5, 6],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Instantiate grid search cross validation object\n",
    "gs_forest = GridSearchCV(forest, params_grid, cv=Kfold);\n",
    "\n",
    "# Fit the model\n",
    "gs_forest.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters from the grid search\n",
    "forest_best_params = gs_forest.best_params_\n",
    "\n",
    "# Instantiate a new model\n",
    "forest = RandomForestClassifier(\n",
    "    n_estimators=forest_best_params.get('n_estimators'), \n",
    "    max_depth=forest_best_params.get('max_depth'), \n",
    "    random_state=32,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the data \n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_forest = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "small intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initiate the Logistic regression model\n",
    "# gbrt = GradientBoostingClassifier(random_state=32)\n",
    "\n",
    "# # Implement stratified Cross Validation\n",
    "# kfold = KFold(n_splits=5)\n",
    "\n",
    "# # Find the best parameters using gridsearch cross validation\n",
    "# params_grid = { \n",
    "#     'n_estimators': [10, 100], 'max_depth': [4, 8], 'learning_rate': [0.1, 0.01, 1]\n",
    "# }\n",
    "\n",
    "# # Instantiate grid search cross validation object\n",
    "# gs_gbrt = GridSearchCV(gbrt, params_grid, cv=Kfold);\n",
    "\n",
    "# # Fit the model\n",
    "# gs_gbrt.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_gbrt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best paramters\n",
    "#gbrt_best_params = gs_gbrt.best_params_\n",
    "\n",
    "# Instantiate a new object\n",
    "gbrt = GradientBoostingClassifier(learning_rate=0.1, max_depth=4)#max_depth=gbrt_best_params.get('max_depth'), n_estimators=gbrt_best_params.get('n_estimators'), learning_rate=gbrt_best_params.get('learning_rate'))\n",
    "\n",
    "# fit data\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_gbrt = gbrt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"><a/>\n",
    "# Evaluation\n",
    "    \n",
    "There are several ways to ways to score classification problems such as:\n",
    "- simple accuracy\n",
    "- jaccard index\n",
    "- f1-scores\n",
    "    \n",
    "\n",
    "For our problem, identifying true positives (true category one accident or true cattegory 2 accident) is more important than true negatives since [COMPLETE]\n",
    "I will also compare my results to chance by using the dummy classifier.\n",
    "For this binary classification problem, I will use precision and recall measurements, associated F1 scores and Receiver Operand Curves (ROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from sklearn.metrics import jaccard_score, f1_score, confusion_matrix, classification_report,\\\n",
    "                            roc_curve, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the different f1 scores for each estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'   Logistic Regression: F1 Score on test set: {f1_score(y_test, y_logreg,): 0.2f}')\n",
    "print(f'Random Forest Ensemble: F1 Score on test set: {f1_score(y_test, y_forest,): 0.2f}')\n",
    "print(f'     Gradient Boosting: F1 Score on test set: {f1_score(y_test, y_gbrt,): 0.2f}')\n",
    "print(f'         Decision Tree: F1 Score on test set: {f1_score(y_test, y_dummy,): 0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To better undertand the F1 scory_gbrt we can have a look at the `classification_report` and corresponding `confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regresion\\n--------------------\\n\", classification_report(y_test, y_logreg))\n",
    "print(\"Random Forest\\n--------------------\\n\", classification_report(y_test, y_forest))\n",
    "print(\"Gradient Boosting\\n---------------------\\n\", classification_report(y_test, y_gbrt))\n",
    "print(\"Decision Tree\\n---------------------\\n\", classification_report(y_test, y_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regresion\\n--------------------\\n\", classification_report(y_test, y_logreg))\n",
    "print(\"Random Forest\\n--------------------\\n\", classification_report(y_test, y_forest))\n",
    "print(\"Gradient Boosting\\n---------------------\\n\", classification_report(y_test, y_gbrt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for idx, (name, estimator) in enumerate({'Logistic Regression': logreg, 'Random Forest': forest, 'Gradient Boosting':gbrt}.items()):\n",
    "    plot_confusion_matrix(estimator, X_test, y_test, ax=ax[idx])\n",
    "    ax[idx].set(title=f'{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_precision_recall_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "for idx, (name, estimator) in enumerate({'Logistic Regression': logreg, 'Random Forest': forest, 'Gradient Boosting':gbrt}.items()):\n",
    "    plot_precision_recall_curve(estimator, X_test, y_test, ax=ax[idx])\n",
    "    ax[idx].set(title=f'{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_roc_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "for idx, (name, estimator) in enumerate({'Logistic Regression': logreg, 'Random Forest': forest, 'Gradient Boosting':gbrt}.items()):\n",
    "    plot_roc_curve(estimator, X_test, y_test, ax=ax[idx])\n",
    "    ax[idx].set(title=f'{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"><a/>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This project resulted in article that can be found [here](link-to-article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
